{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import tensorcircuit as tc\n",
    "from opt_einsum import contract\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "import optax\n",
    "\n",
    "from src.QDDPM_jax import QDDPM, HaarSampleGeneration\n",
    "from src.distance_jax import naturalDistance, WassDistance, sinkhornDistance\n",
    "\n",
    "rc('text', usetex=True)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check device\n",
    "print(jax.lib.xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, epochs, dis_measure='wd'):\n",
    "    '''\n",
    "    training for the backward PQC at step t using whole dataset\n",
    "    Args:\n",
    "    model: QDDPM model\n",
    "    t: diffusion step\n",
    "    params_tot: collection of PQC parameters for steps > t \n",
    "    epochs: number of iterations\n",
    "    dis_measure: the distance measure to compare two distributions of quantum states\n",
    "    dis_params: potential hyper-parameters for distance measure\n",
    "    '''\n",
    "    Ndata = inputs_T.shape[0]\n",
    "\n",
    "    input_tplus1 = model.prepareInput_t(\n",
    "        inputs_T, params_tot, t, Ndata)  # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = []  # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    key = random.PRNGKey(np.random.randint(low=0, high=10000))\n",
    "    param_shape = 2 * model.n_tot * model.L\n",
    "    params_t = random.normal(key, shape=(param_shape, ))\n",
    "\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = optax.adam(learning_rate=5e-4)\n",
    "    opt_state = optimizer.init(params_t)\n",
    "\n",
    "    if dis_measure == 'nat':\n",
    "        def loss_func(params_t, input_tplus1, true_data):\n",
    "            output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "            loss = naturalDistance(output_t, true_data)\n",
    "\n",
    "            return loss\n",
    "\n",
    "    elif dis_measure == 'wd':\n",
    "        def loss_func(params_t, input_tplus1, true_data):\n",
    "            output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "            loss = sinkhornDistance(output_t, true_data, reg=0.01)\n",
    "\n",
    "            return loss\n",
    "\n",
    "    loss_func_vg = jax.jit(jax.value_and_grad(loss_func))\n",
    "    # @partial(jax.jit, static_argnums=(2, ))\n",
    "\n",
    "    def update(params_t, input_tplus1, true_data, opt_state):\n",
    "        loss_value, grads = loss_func_vg(params_t, input_tplus1, true_data)\n",
    "        updates, new_opt_state = optimizer.update(grads, opt_state, params_t)\n",
    "        new_params_t = optax.apply_updates(params_t, updates)\n",
    "\n",
    "        return new_params_t, new_opt_state, loss_value\n",
    "\n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        np.random.seed()\n",
    "        indices = np.random.choice(\n",
    "            states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        params_t, opt_state, loss_value = update(\n",
    "            params_t, input_tplus1, true_data, opt_state)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"Step {}, loss: {:.7f}, time elapsed: {:.4f} seconds\".format(\n",
    "                step, loss_value, time.time() - t0))\n",
    "\n",
    "        loss_hist.append(loss_value)  # record the current loss\n",
    "\n",
    "    return params_t, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, na = 6, 4\n",
    "T = 20\n",
    "L = 12\n",
    "\n",
    "Ndata = 10000\n",
    "epochs = 1201\n",
    "\n",
    "inputs_T = HaarSampleGeneration(Ndata, 2 ** n, seed=42)\n",
    "states_diff = jnp.load('data/cluster/cluster0Diff_n6T%d_N10000.npy' % T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "params_tot = jnp.zeros((T, 2 * (n + na) * L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 0.0015049, time elapsed: 25.7046 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\GitHub\\QuantGenMdl\\training_jax.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m params, loss \u001b[39m=\u001b[39m Training_t(model, T \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, inputs_T, params_tot, epochs, \u001b[39m'\u001b[39m\u001b[39mnat\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32me:\\GitHub\\QuantGenMdl\\training_jax.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     states_diff\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], size\u001b[39m=\u001b[39mNdata, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m true_data \u001b[39m=\u001b[39m states_diff[t, indices]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m params_t, opt_state, loss_value \u001b[39m=\u001b[39m update(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     params_t, input_tplus1, true_data, opt_state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStep \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{:.7f}\u001b[39;00m\u001b[39m, time elapsed: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         step, loss_value, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0))\n",
      "\u001b[1;32me:\\GitHub\\QuantGenMdl\\training_jax.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(params_t, input_tplus1, true_data, opt_state):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     loss_value, grads \u001b[39m=\u001b[39m loss_func_vg(params_t, input_tplus1, true_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     updates, new_opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mupdate(grads, opt_state, params_t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GitHub/QuantGenMdl/training_jax.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     new_params_t \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mapply_updates(params_t, updates)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params, loss = Training_t(model, T - 1, inputs_T, params_tot, epochs, 'nat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
