{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 02:43:13.614755: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-29 02:43:13.644199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-29 02:43:14.066646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-29 02:43:14.566001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:14.600730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:14.600774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:14.605549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:14.605589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:14.605604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:15.163822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:15.163878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:15.163883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-29 02:43:15.163900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 02:43:15.163941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21598 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/francishsu/miniconda3/lib/python3.11/site-packages/ot/backend.py:2998: UserWarning: To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: \n",
      "from tensorflow.python.ops.numpy_ops import np_config\n",
      "np_config.enable_numpy_behavior()\n",
      "  register_backend(TensorflowBackend())\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import unitary_group\n",
    "from opt_einsum import contract\n",
    "\n",
    "from QDDPM_tf import MultiQubitDiffusionModel, QDDPM\n",
    "from QDDPM_tf import naturalDistance, WassDistance\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlated noise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseTraining_t(model, t, inputs_T, params_tot, Ndata, epochs, dis_measure='nat', dis_params={}):\n",
    "    '''\n",
    "    backward training of QDDPM for correlated noise\n",
    "    Args:\n",
    "    model: the QDDPM\n",
    "    t: diffusion step\n",
    "    params_tot: collection of PQC parameters for steps > t \n",
    "    Ndata: number of samples in training data set\n",
    "    epochs: number of iterations\n",
    "    dis_measure: the distance measure to compare two distributions of quantum states\n",
    "    dis_params: potential hyper-parameters for distance measure\n",
    "    '''\n",
    "    \n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "    f_hist = [] # record of std of bloch-y cooredinates\n",
    "\n",
    "    tf.random.set_seed(None)\n",
    "    params_t = tf.Variable(tf.random.normal([2 * model.n_tot * model.L]))\n",
    "    # set optimizer and learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.01, 200, 0.8, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "            if dis_measure == 'nat':\n",
    "                # natural distance\n",
    "                loss = naturalDistance(output_t, true_data)\n",
    "            elif dis_measure == 'wd':\n",
    "                # Wassastein distance\n",
    "                loss = WassDistance(output_t, true_data)\n",
    "\n",
    "        grads = tape.gradient(loss, [params_t])\n",
    "        optimizer.apply_gradients(zip(grads, [params_t]))\n",
    "\n",
    "        loss_hist.append(tf.stop_gradient(loss)) # record the current loss\n",
    "        f_hist.append(tf.math.reduce_mean(tf.abs(tf.stop_gradient(output_t)[:,2])**2))\n",
    "\n",
    "    return tf.stop_gradient(params_t), tf.squeeze(tf.stack(loss_hist)), tf.squeeze(tf.stack(f_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "n, na = 2, 2\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 500\n",
    "epochs = 2500\n",
    "repeat = 5\n",
    "method = 'nat'\n",
    "\n",
    "diffModel = MultiQubitDiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed=22)\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "states_diff = np.load('data/corrNoiseDiff_n%dT%d_N5000.npy'%(n, T))\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "data_path = \"noise/record_%s/\" %method\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "for t in range(T-1, -1, -1):\n",
    "    params_tot = np.zeros((20, 2*(n+na)*L))\n",
    "    for tt in range(t+1, 20):\n",
    "        params_tot[tt] = np.load('noise/record_%s/QDDPMcorrNoiseparams_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "                                % (method, n, na, T, L, tt, method))\n",
    "\n",
    "    params_all = np.zeros((repeat, 2*(n+na)**L))\n",
    "    loss_all = np.zeros((repeat, epochs))\n",
    "    f_all = np.zeros((repeat, epochs))\n",
    "    for trial in range(repeat):\n",
    "        params, loss, f = noiseTraining_t(\n",
    "            model, t, inputs_T, params_tot, Ndata, epochs, method)\n",
    "        params_all[trial] = params.numpy()\n",
    "        loss_all[trial] = loss.numpy()\n",
    "        f_all[trial] = f.numpy()\n",
    "        print(t, trial, loss_all[trial, -1])\n",
    "\n",
    "    idx = np.argmin(loss_all[:, -1])\n",
    "    np.save('noise/record_%s/QDDPMcorrNoiseparams_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "            % (method, n, na, T, L, t, method), params_all[idx])\n",
    "    np.save('noise/record_%s/QDDPMcorrNoiseloss_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "            % (method, n, na, T, L, t, method), loss_all[idx])\n",
    "    np.save('noise/record_%s/QDDPMcorrNoisef_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "            % (method, n, na, T, L, t, method), f_all[idx])\n",
    "\n",
    "    print('corr-noise, na=%d, t=%d, min loss=%s, F10=%s'\n",
    "          % (na, t, loss_all[idx, -1], f_all[idx, -1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identical product of circle training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idProdTraining_t(model, t, inputs_T, params_tot, Ndata, epochs, dis_measure='wd', dis_params={}):\n",
    "    '''\n",
    "    backward training of QDDPM at step t for identical product state\n",
    "    Args:\n",
    "    model: the QDDPM\n",
    "    t: diffusion step\n",
    "    params_tot: collection of PQC parameters for steps > t \n",
    "    Ndata: number of samples in training data set\n",
    "    epochs: number of iterations\n",
    "    dis_measure: the distance measure to compare two distributions of quantum states\n",
    "    dis_params: potential hyper-parameters for distance measure\n",
    "    '''\n",
    "    \n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "    y1_hist = [] # record of bloch-y coordinates for first qubit\n",
    "    y2_hist = [] # record of bloch-y coordinates for second qubit\n",
    "    # Pauli-y operator on first/second qubit\n",
    "    sy = np.array([[0,-1j], [1j, 0]])\n",
    "    sy1 = tf.cast(tf.convert_to_tensor(np.kron(sy, np.eye(2))), dtype = tf.complex64)\n",
    "    sy2 = tf.cast(tf.convert_to_tensor(np.kron(np.eye(2), sy)), dtype = tf.complex64)\n",
    "\n",
    "    if dis_measure == 'nat':\n",
    "        # natural distance\n",
    "        loss_func = naturalDistance\n",
    "    elif dis_measure == 'wd':\n",
    "        # Wassastein distance\n",
    "        loss_func = WassDistance\n",
    "\n",
    "    tf.random.set_seed(None)\n",
    "    params_t = tf.Variable(tf.random.normal([2 * model.n_tot * model.L]))\n",
    "    # set optimizer and learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.01, 200, 0.8, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "            loss = loss_func(output_t, true_data)\n",
    "\n",
    "        grads = tape.gradient(loss, [params_t])\n",
    "        optimizer.apply_gradients(zip(grads, [params_t]))\n",
    "\n",
    "        loss_hist.append(tf.stop_gradient(loss)) # record the current loss\n",
    "        y1 = tfm.real(contract('mi, ij, mj-> m',  tfm.conj(tf.stop_gradient(output_t)), sy1, tf.stop_gradient(output_t)))\n",
    "        y2 = tfm.real(contract('mi, ij, mj-> m',  tfm.conj(tf.stop_gradient(output_t)), sy2, tf.stop_gradient(output_t)))\n",
    "        y1_hist.append(y1)\n",
    "        y2_hist.append(y2)\n",
    "\n",
    "    y1_hist = tf.stack(y1_hist)\n",
    "    y2_hist = tf.stack(y2_hist)\n",
    "    \n",
    "    return tf.stop_gradient(params_t), tf.squeeze(tf.stack(loss_hist)), y1_hist, y2_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prodHaarStates(n, Ndata, seed):\n",
    "    '''\n",
    "    generate two-qubit product states of one-qubit Haar random states\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    state_qs = unitary_group.rvs(2, size=n*Ndata)[:, :, 0]\n",
    "    state_qs = np.split(state_qs, n, axis=0)\n",
    "    states = state_qs[0]\n",
    "    for i in range(1, n):\n",
    "        states = contract('mi, mj->mij', states, state_qs[i]).reshape((Ndata, 2**(i + 1)))\n",
    "        \n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 0.0013423562049865723 173.09123904599983\n",
      "19 1 0.00344240665435791 336.44428790800157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/francishsu/github/training_task.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m start_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repeat):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     params, loss, y1, y2 \u001b[39m=\u001b[39m idProdTraining_t(model, t, inputs_T, params_tot, Ndata, epochs, method)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     params_all[trial] \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     loss_all[trial] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m/home/francishsu/github/training_task.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     output_t \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbackwardOutput_t(input_tplus1, params_t)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_func(output_t, true_data)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, [params_t])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, [params_t]))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/francishsu/github/training_task.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m loss_hist\u001b[39m.\u001b[39mappend(tf\u001b[39m.\u001b[39mstop_gradient(loss)) \u001b[39m# record the current loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39mimperative_grad(\n\u001b[1;32m   1064\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape,\n\u001b[1;32m   1065\u001b[0m     flat_targets,\n\u001b[1;32m   1066\u001b[0m     flat_sources,\n\u001b[1;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39moutput_gradients,\n\u001b[1;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39mflat_sources_raw,\n\u001b[1;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39munconnected_gradients)\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39mas_str(unconnected_gradients\u001b[39m.\u001b[39mvalue))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:789\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function.<locals>._backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    787\u001b[0m   \u001b[39mif\u001b[39;00m input_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m backward_function_inputs:\n\u001b[1;32m    788\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m \u001b[39mreturn\u001b[39;00m backward\u001b[39m.\u001b[39m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     processed_args, remapped_captures)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function(\u001b[39m*\u001b[39margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, na = 2, 4\n",
    "T = 20\n",
    "L = 10\n",
    "Ndata = 500\n",
    "epochs = 2500\n",
    "repeat = 5\n",
    "method = 'wd'\n",
    "\n",
    "inputs_T = prodHaarStates(n, Ndata, seed=22)\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "states_diff = np.load('data/idProdDiff_n%dT%d_N5000.npy'%(n, T))\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "data_path = \"product/record_%s/\" %method\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "for t in range(T-1, -1, -1):\n",
    "    params_tot = np.zeros((20, 2*(n+na)*L))\n",
    "    for tt in range(t+1, 20):\n",
    "        params_tot[tt] = np.load('product/record_%s/QDDPMidProdparams_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "                                %(method, n, na, T, L, tt, method))\n",
    "\n",
    "    params_all = np.zeros((repeat, 2*(n + na)*L))\n",
    "    loss_all = np.zeros((repeat, epochs))\n",
    "    y1_all = np.zeros((repeat, epochs, Ndata))\n",
    "    y2_all = np.zeros((repeat, epochs, Ndata))\n",
    "\n",
    "    start_time = timer()\n",
    "    for trial in range(repeat):\n",
    "        params, loss, y1, y2 = idProdTraining_t(model, t, inputs_T, params_tot, Ndata, epochs, method)\n",
    "        params_all[trial] = params.numpy()\n",
    "        loss_all[trial] = loss.numpy()\n",
    "        y1_all[trial] = y1.numpy()\n",
    "        y2_all[trial] = y2.numpy()\n",
    "        curr_time = timer()\n",
    "        print(t, trial, loss_all[trial, -1], curr_time - start_time)\n",
    "        \n",
    "    idx = np.argmin(loss_all[:, -1])\n",
    "    np.save('product/record_%s/QDDPMidProdparams_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "            % (method, n, na, T, L, t, method), params_all[idx])\n",
    "    np.save('product/record_%s/QDDPMidProdloss_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "            % (method, n, na, T, L, t, method), loss_all[idx])\n",
    "    np.save('product/record_%s/QDDPMidPrody1_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "            % (method, n, na, T, L, t, method), y1_all[idx])\n",
    "    np.save('product/record_%s/QDDPMidPrody2_n%dna%dT%dL%d_t%d_%s.npy'\n",
    "            % (method, n, na, T, L, t, method), y2_all[idx])\n",
    "\n",
    "    print('id-product, na=%d, t=%d, min loss=%s, (y1-y2)^2=%s'\n",
    "          % (na, t, loss_all[idx, -1], np.mean((y1_all[idx, -1]-y2_all[idx, -1])**2)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area-law entanglement training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorcircuit_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
